2024-05-10 16:03:48,792 Progressive Transformers for End-to-End SLP
2024-05-10 16:03:48,801 cfg.data.src                       : gloss
2024-05-10 16:03:48,801 cfg.data.trg                       : skels
2024-05-10 16:03:48,801 cfg.data.files                     : files
2024-05-10 16:03:48,801 cfg.data.train                     : ./data/train/raw/openpose_output/json/
2024-05-10 16:03:48,801 cfg.data.dev                       : ./data/val/raw/openpose_output/json/
2024-05-10 16:03:48,801 cfg.data.test                      : ./data/test/raw/openpose_output/json/
2024-05-10 16:03:48,801 cfg.data.max_sent_length           : 300
2024-05-10 16:03:48,801 cfg.data.skip_frames               : 1
2024-05-10 16:03:48,801 cfg.data.src_vocab                 : ./configs/src_vocab.txt
2024-05-10 16:03:48,801 cfg.training.random_seed           : 27
2024-05-10 16:03:48,801 cfg.training.optimizer             : adam
2024-05-10 16:03:48,801 cfg.training.learning_rate         : 0.001
2024-05-10 16:03:48,801 cfg.training.learning_rate_min     : 0.0002
2024-05-10 16:03:48,801 cfg.training.weight_decay          : 0.0
2024-05-10 16:03:48,801 cfg.training.clip_grad_norm        : 5.0
2024-05-10 16:03:48,801 cfg.training.batch_size            : 8
2024-05-10 16:03:48,801 cfg.training.scheduling            : plateau
2024-05-10 16:03:48,801 cfg.training.patience              : 7
2024-05-10 16:03:48,801 cfg.training.decrease_factor       : 0.7
2024-05-10 16:03:48,801 cfg.training.early_stopping_metric : dtw
2024-05-10 16:03:48,801 cfg.training.epochs                : 500
2024-05-10 16:03:48,801 cfg.training.validation_freq       : 10
2024-05-10 16:03:48,801 cfg.training.logging_freq          : 250
2024-05-10 16:03:48,801 cfg.training.eval_metric           : dtw
2024-05-10 16:03:48,801 cfg.training.model_dir             : ./Models/Base
2024-05-10 16:03:48,801 cfg.training.overwrite             : True
2024-05-10 16:03:48,801 cfg.training.continue              : False
2024-05-10 16:03:48,801 cfg.training.shuffle               : True
2024-05-10 16:03:48,801 cfg.training.use_cuda              : False
2024-05-10 16:03:48,801 cfg.training.max_output_length     : 420
2024-05-10 16:03:48,801 cfg.training.keep_last_ckpts       : 1
2024-05-10 16:03:48,801 cfg.training.loss                  : MSE
2024-05-10 16:03:48,801 cfg.model.initializer              : xavier
2024-05-10 16:03:48,801 cfg.model.bias_initializer         : zeros
2024-05-10 16:03:48,801 cfg.model.embed_initializer        : xavier
2024-05-10 16:03:48,801 cfg.model.trg_size                 : 412
2024-05-10 16:03:48,801 cfg.model.just_count_in            : False
2024-05-10 16:03:48,802 cfg.model.gaussian_noise           : False
2024-05-10 16:03:48,802 cfg.model.noise_rate               : 5
2024-05-10 16:03:48,802 cfg.model.future_prediction        : 0
2024-05-10 16:03:48,802 cfg.model.encoder.type             : transformer
2024-05-10 16:03:48,802 cfg.model.encoder.num_layers       : 2
2024-05-10 16:03:48,802 cfg.model.encoder.num_heads        : 4
2024-05-10 16:03:48,802 cfg.model.encoder.embeddings.embedding_dim : 512
2024-05-10 16:03:48,802 cfg.model.encoder.embeddings.dropout : 0.0
2024-05-10 16:03:48,802 cfg.model.encoder.hidden_size      : 512
2024-05-10 16:03:48,802 cfg.model.encoder.ff_size          : 2048
2024-05-10 16:03:48,802 cfg.model.encoder.dropout          : 0.0
2024-05-10 16:03:48,802 cfg.model.decoder.type             : transformer
2024-05-10 16:03:48,802 cfg.model.decoder.num_layers       : 2
2024-05-10 16:03:48,802 cfg.model.decoder.num_heads        : 4
2024-05-10 16:03:48,802 cfg.model.decoder.embeddings.embedding_dim : 512
2024-05-10 16:03:48,802 cfg.model.decoder.embeddings.dropout : 0.0
2024-05-10 16:03:48,802 cfg.model.decoder.hidden_size      : 512
2024-05-10 16:03:48,802 cfg.model.decoder.ff_size          : 2048
2024-05-10 16:03:48,802 cfg.model.decoder.dropout          : 0.0
2024-05-10 16:03:48,945 EPOCH 1
2024-05-10 16:03:50,066 Epoch   1: total training loss 1.88167
2024-05-10 16:03:50,066 EPOCH 2
2024-05-10 16:03:50,668 Epoch   2: total training loss 1.28583
2024-05-10 16:03:50,668 EPOCH 3
2024-05-10 16:03:51,248 Epoch   3: total training loss 0.33711
2024-05-10 16:03:51,249 EPOCH 4
2024-05-10 16:03:51,827 Epoch   4: total training loss 0.22342
2024-05-10 16:03:51,827 EPOCH 5
2024-05-10 16:03:52,398 Epoch   5: total training loss 0.10121
2024-05-10 16:03:52,399 EPOCH 6
2024-05-10 16:03:52,959 Epoch   6: total training loss 0.08835
2024-05-10 16:03:52,959 EPOCH 7
2024-05-10 16:03:53,526 Epoch   7: total training loss 0.08035
2024-05-10 16:03:53,526 EPOCH 8
2024-05-10 16:03:54,098 Epoch   8: total training loss 0.07987
2024-05-10 16:03:54,098 EPOCH 9
2024-05-10 16:03:54,661 Epoch   9: total training loss 0.07235
2024-05-10 16:03:54,662 EPOCH 10
2024-05-10 16:03:55,388 Epoch  10: total training loss 0.06188
2024-05-10 16:03:55,388 EPOCH 11
2024-05-10 16:03:55,966 Epoch  11: total training loss 0.05013
2024-05-10 16:03:55,966 EPOCH 12
2024-05-10 16:03:56,534 Epoch  12: total training loss 0.03676
2024-05-10 16:03:56,534 EPOCH 13
2024-05-10 16:03:57,107 Epoch  13: total training loss 0.02586
2024-05-10 16:03:57,107 EPOCH 14
2024-05-10 16:03:57,669 Epoch  14: total training loss 0.02203
2024-05-10 16:03:57,669 EPOCH 15
2024-05-10 16:03:58,224 Epoch  15: total training loss 0.02423
2024-05-10 16:03:58,224 EPOCH 16
2024-05-10 16:03:58,782 Epoch  16: total training loss 0.02731
2024-05-10 16:03:58,782 EPOCH 17
2024-05-10 16:03:59,353 Epoch  17: total training loss 0.02770
2024-05-10 16:03:59,353 EPOCH 18
2024-05-10 16:03:59,908 Epoch  18: total training loss 0.02563
2024-05-10 16:03:59,908 EPOCH 19
2024-05-10 16:04:00,618 Epoch  19: total training loss 0.02282
2024-05-10 16:04:00,618 EPOCH 20
2024-05-10 16:04:01,212 Epoch  20: total training loss 0.02002
2024-05-10 16:04:01,212 EPOCH 21
2024-05-10 16:04:01,777 Epoch  21: total training loss 0.01714
2024-05-10 16:04:01,777 EPOCH 22
2024-05-10 16:04:02,338 Epoch  22: total training loss 0.01450
2024-05-10 16:04:02,338 EPOCH 23
2024-05-10 16:04:02,911 Epoch  23: total training loss 0.01289
2024-05-10 16:04:02,912 EPOCH 24
2024-05-10 16:04:03,476 Epoch  24: total training loss 0.01255
2024-05-10 16:04:03,476 EPOCH 25
2024-05-10 16:04:04,039 Epoch  25: total training loss 0.01278
2024-05-10 16:04:04,039 EPOCH 26
2024-05-10 16:04:04,642 Epoch  26: total training loss 0.01264
2024-05-10 16:04:04,642 EPOCH 27
2024-05-10 16:04:05,217 Epoch  27: total training loss 0.01192
2024-05-10 16:04:05,217 EPOCH 28
2024-05-10 16:04:05,786 Epoch  28: total training loss 0.01106
2024-05-10 16:04:05,786 EPOCH 29
2024-05-10 16:04:06,346 Epoch  29: total training loss 0.01044
2024-05-10 16:04:06,346 EPOCH 30
2024-05-10 16:04:06,930 Epoch  30: total training loss 0.00995
2024-05-10 16:04:06,930 EPOCH 31
2024-05-10 16:04:07,657 Epoch  31: total training loss 0.00931
2024-05-10 16:04:07,657 EPOCH 32
2024-05-10 16:04:08,215 Epoch  32: total training loss 0.00857
2024-05-10 16:04:08,215 EPOCH 33
2024-05-10 16:04:08,778 Epoch  33: total training loss 0.00803
2024-05-10 16:04:08,778 EPOCH 34
2024-05-10 16:04:09,338 Epoch  34: total training loss 0.00779
2024-05-10 16:04:09,338 EPOCH 35
2024-05-10 16:04:09,896 Epoch  35: total training loss 0.00767
2024-05-10 16:04:09,896 EPOCH 36
2024-05-10 16:04:10,451 Epoch  36: total training loss 0.00742
2024-05-10 16:04:10,451 EPOCH 37
2024-05-10 16:04:11,007 Epoch  37: total training loss 0.00707
2024-05-10 16:04:11,007 EPOCH 38
2024-05-10 16:04:11,562 Epoch  38: total training loss 0.00681
2024-05-10 16:04:11,562 EPOCH 39
2024-05-10 16:04:12,136 Epoch  39: total training loss 0.00669
2024-05-10 16:04:12,136 EPOCH 40
2024-05-10 16:04:12,702 Epoch  40: total training loss 0.00657
2024-05-10 16:04:12,702 EPOCH 41
2024-05-10 16:04:13,255 Epoch  41: total training loss 0.00636
2024-05-10 16:04:13,255 EPOCH 42
2024-05-10 16:04:13,836 Epoch  42: total training loss 0.00610
2024-05-10 16:04:13,836 EPOCH 43
2024-05-10 16:04:14,465 Epoch  43: total training loss 0.00594
2024-05-10 16:04:14,466 EPOCH 44
2024-05-10 16:04:15,057 Epoch  44: total training loss 0.00588
2024-05-10 16:04:15,057 EPOCH 45
2024-05-10 16:04:15,706 Epoch  45: total training loss 0.00581
2024-05-10 16:04:15,706 EPOCH 46
2024-05-10 16:04:16,258 Epoch  46: total training loss 0.00566
2024-05-10 16:04:16,258 EPOCH 47
2024-05-10 16:04:16,814 Epoch  47: total training loss 0.00549
2024-05-10 16:04:16,814 EPOCH 48
2024-05-10 16:04:17,378 Epoch  48: total training loss 0.00539
2024-05-10 16:04:17,378 EPOCH 49
2024-05-10 16:04:17,938 Epoch  49: total training loss 0.00536
2024-05-10 16:04:17,939 EPOCH 50
2024-05-10 16:04:18,491 Epoch  50: total training loss 0.00534
2024-05-10 16:04:18,491 EPOCH 51
2024-05-10 16:04:19,056 Epoch  51: total training loss 0.00528
2024-05-10 16:04:19,056 EPOCH 52
2024-05-10 16:04:19,615 Epoch  52: total training loss 0.00522
2024-05-10 16:04:19,615 EPOCH 53
2024-05-10 16:04:20,179 Epoch  53: total training loss 0.00517
2024-05-10 16:04:20,179 EPOCH 54
2024-05-10 16:04:20,748 Epoch  54: total training loss 0.00512
2024-05-10 16:04:20,748 EPOCH 55
2024-05-10 16:04:21,329 Epoch  55: total training loss 0.00504
2024-05-10 16:04:21,329 EPOCH 56
2024-05-10 16:04:21,904 Epoch  56: total training loss 0.00496
2024-05-10 16:04:21,904 EPOCH 57
2024-05-10 16:04:22,459 Epoch  57: total training loss 0.00493
2024-05-10 16:04:22,459 EPOCH 58
2024-05-10 16:04:23,017 Epoch  58: total training loss 0.00493
2024-05-10 16:04:23,017 EPOCH 59
2024-05-10 16:04:23,579 Epoch  59: total training loss 0.00493
2024-05-10 16:04:23,580 EPOCH 60
2024-05-10 16:04:24,136 Epoch  60: total training loss 0.00491
2024-05-10 16:04:24,136 EPOCH 61
2024-05-10 16:04:24,689 Epoch  61: total training loss 0.00488
2024-05-10 16:04:24,689 EPOCH 62
2024-05-10 16:04:25,247 Epoch  62: total training loss 0.00486
2024-05-10 16:04:25,247 EPOCH 63
2024-05-10 16:04:25,811 Epoch  63: total training loss 0.00483
2024-05-10 16:04:25,811 EPOCH 64
2024-05-10 16:04:26,391 Epoch  64: total training loss 0.00480
2024-05-10 16:04:26,391 EPOCH 65
2024-05-10 16:04:26,950 Epoch  65: total training loss 0.00476
2024-05-10 16:04:26,950 EPOCH 66
2024-05-10 16:04:27,699 Epoch  66: total training loss 0.00475
2024-05-10 16:04:27,699 EPOCH 67
2024-05-10 16:04:28,289 Epoch  67: total training loss 0.00476
2024-05-10 16:04:28,289 EPOCH 68
2024-05-10 16:04:28,874 Epoch  68: total training loss 0.00476
2024-05-10 16:04:28,875 EPOCH 69
2024-05-10 16:04:29,439 Epoch  69: total training loss 0.00475
2024-05-10 16:04:29,439 EPOCH 70
2024-05-10 16:04:30,013 Epoch  70: total training loss 0.00474
2024-05-10 16:04:30,013 EPOCH 71
2024-05-10 16:04:30,581 Epoch  71: total training loss 0.00472
2024-05-10 16:04:30,581 EPOCH 72
2024-05-10 16:04:31,141 Epoch  72: total training loss 0.00471
2024-05-10 16:04:31,142 EPOCH 73
2024-05-10 16:04:31,789 Epoch  73: total training loss 0.00469
2024-05-10 16:04:31,789 EPOCH 74
2024-05-10 16:04:32,395 Epoch  74: total training loss 0.00469
2024-05-10 16:04:32,395 EPOCH 75
2024-05-10 16:04:32,998 Epoch  75: total training loss 0.00469
2024-05-10 16:04:32,998 EPOCH 76
2024-05-10 16:04:33,568 Epoch  76: total training loss 0.00469
2024-05-10 16:04:33,568 EPOCH 77
2024-05-10 16:04:34,142 Epoch  77: total training loss 0.00468
2024-05-10 16:04:34,142 EPOCH 78
2024-05-10 16:04:34,703 Epoch  78: total training loss 0.00468
2024-05-10 16:04:34,703 EPOCH 79
2024-05-10 16:04:35,268 Epoch  79: total training loss 0.00467
2024-05-10 16:04:35,268 EPOCH 80
2024-05-10 16:04:35,826 Epoch  80: total training loss 0.00467
2024-05-10 16:04:35,826 EPOCH 81
2024-05-10 16:04:36,373 Epoch  81: total training loss 0.00466
2024-05-10 16:04:36,374 EPOCH 82
2024-05-10 16:04:36,939 Epoch  82: total training loss 0.00466
2024-05-10 16:04:36,939 EPOCH 83
2024-05-10 16:04:37,497 Epoch  83: total training loss 0.00466
2024-05-10 16:04:37,497 EPOCH 84
2024-05-10 16:04:38,057 Epoch  84: total training loss 0.00466
2024-05-10 16:04:38,057 EPOCH 85
2024-05-10 16:04:38,617 Epoch  85: total training loss 0.00465
2024-05-10 16:04:38,618 EPOCH 86
2024-05-10 16:04:39,172 Epoch  86: total training loss 0.00465
2024-05-10 16:04:39,172 EPOCH 87
2024-05-10 16:04:39,744 Epoch  87: total training loss 0.00465
2024-05-10 16:04:39,744 EPOCH 88
2024-05-10 16:04:40,327 Epoch  88: total training loss 0.00465
2024-05-10 16:04:40,327 EPOCH 89
2024-05-10 16:04:40,890 Epoch  89: total training loss 0.00464
2024-05-10 16:04:40,890 EPOCH 90
2024-05-10 16:04:41,453 Epoch  90: total training loss 0.00464
2024-05-10 16:04:41,453 EPOCH 91
2024-05-10 16:04:42,016 Epoch  91: total training loss 0.00464
2024-05-10 16:04:42,017 EPOCH 92
2024-05-10 16:04:42,587 Epoch  92: total training loss 0.00464
2024-05-10 16:04:42,587 EPOCH 93
2024-05-10 16:04:43,146 Epoch  93: total training loss 0.00464
2024-05-10 16:04:43,146 EPOCH 94
2024-05-10 16:04:43,733 Epoch  94: total training loss 0.00464
2024-05-10 16:04:43,733 EPOCH 95
2024-05-10 16:04:44,296 Epoch  95: total training loss 0.00464
2024-05-10 16:04:44,296 EPOCH 96
2024-05-10 16:04:44,855 Epoch  96: total training loss 0.00463
2024-05-10 16:04:44,855 EPOCH 97
2024-05-10 16:04:45,414 Epoch  97: total training loss 0.00463
2024-05-10 16:04:45,414 EPOCH 98
2024-05-10 16:04:45,975 Epoch  98: total training loss 0.00463
2024-05-10 16:04:45,976 EPOCH 99
2024-05-10 16:04:46,530 Epoch  99: total training loss 0.00463
2024-05-10 16:04:46,530 EPOCH 100
2024-05-10 16:04:47,088 Epoch 100: total training loss 0.00463
2024-05-10 16:04:47,088 EPOCH 101
2024-05-10 16:04:47,650 Epoch 101: total training loss 0.00463
2024-05-10 16:04:47,651 EPOCH 102
2024-05-10 16:04:48,209 Epoch 102: total training loss 0.00463
2024-05-10 16:04:48,209 EPOCH 103
2024-05-10 16:04:48,762 Epoch 103: total training loss 0.00463
2024-05-10 16:04:48,762 EPOCH 104
2024-05-10 16:04:49,314 Epoch 104: total training loss 0.00463
2024-05-10 16:04:49,314 EPOCH 105
2024-05-10 16:04:49,870 Epoch 105: total training loss 0.00462
2024-05-10 16:04:49,870 EPOCH 106
2024-05-10 16:04:50,437 Epoch 106: total training loss 0.00462
2024-05-10 16:04:50,437 EPOCH 107
2024-05-10 16:04:51,003 Epoch 107: total training loss 0.00462
2024-05-10 16:04:51,003 EPOCH 108
2024-05-10 16:04:51,573 Epoch 108: total training loss 0.00462
2024-05-10 16:04:51,574 EPOCH 109
2024-05-10 16:04:52,170 Epoch 109: total training loss 0.00462
2024-05-10 16:04:52,170 EPOCH 110
2024-05-10 16:04:52,763 Epoch 110: total training loss 0.00462
2024-05-10 16:04:52,763 EPOCH 111
2024-05-10 16:04:53,328 Epoch 111: total training loss 0.00462
2024-05-10 16:04:53,328 EPOCH 112
2024-05-10 16:04:53,890 Epoch 112: total training loss 0.00462
2024-05-10 16:04:53,890 EPOCH 113
2024-05-10 16:04:54,446 Epoch 113: total training loss 0.00462
2024-05-10 16:04:54,446 EPOCH 114
2024-05-10 16:04:55,002 Epoch 114: total training loss 0.00462
2024-05-10 16:04:55,002 EPOCH 115
2024-05-10 16:04:55,563 Epoch 115: total training loss 0.00461
2024-05-10 16:04:55,563 EPOCH 116
2024-05-10 16:04:56,124 Epoch 116: total training loss 0.00461
2024-05-10 16:04:56,124 EPOCH 117
2024-05-10 16:04:56,680 Epoch 117: total training loss 0.00461
2024-05-10 16:04:56,680 EPOCH 118
2024-05-10 16:04:57,241 Epoch 118: total training loss 0.00461
2024-05-10 16:04:57,241 EPOCH 119
2024-05-10 16:04:57,812 Epoch 119: total training loss 0.00461
2024-05-10 16:04:57,812 EPOCH 120
2024-05-10 16:04:58,397 Epoch 120: total training loss 0.00461
2024-05-10 16:04:58,397 EPOCH 121
2024-05-10 16:04:59,176 Epoch 121: total training loss 0.00461
2024-05-10 16:04:59,177 EPOCH 122
2024-05-10 16:04:59,988 Epoch 122: total training loss 0.00461
2024-05-10 16:04:59,988 EPOCH 123
2024-05-10 16:05:00,593 Epoch 123: total training loss 0.00461
2024-05-10 16:05:00,593 EPOCH 124
2024-05-10 16:05:01,211 Epoch 124: total training loss 0.00461
2024-05-10 16:05:01,211 EPOCH 125
2024-05-10 16:05:01,785 Epoch 125: total training loss 0.00461
2024-05-10 16:05:01,785 EPOCH 126
2024-05-10 16:05:02,353 Epoch 126: total training loss 0.00460
2024-05-10 16:05:02,353 EPOCH 127
2024-05-10 16:05:02,916 Epoch 127: total training loss 0.00460
2024-05-10 16:05:02,917 EPOCH 128
2024-05-10 16:05:03,471 Epoch 128: total training loss 0.00460
2024-05-10 16:05:03,471 EPOCH 129
2024-05-10 16:05:04,047 Epoch 129: total training loss 0.00460
2024-05-10 16:05:04,047 EPOCH 130
2024-05-10 16:05:04,606 Epoch 130: total training loss 0.00460
2024-05-10 16:05:04,606 EPOCH 131
2024-05-10 16:05:05,160 Epoch 131: total training loss 0.00460
2024-05-10 16:05:05,160 EPOCH 132
2024-05-10 16:05:05,719 Epoch 132: total training loss 0.00460
2024-05-10 16:05:05,720 EPOCH 133
2024-05-10 16:05:06,282 Epoch 133: total training loss 0.00460
2024-05-10 16:05:06,282 EPOCH 134
2024-05-10 16:05:06,836 Epoch 134: total training loss 0.00460
2024-05-10 16:05:06,837 EPOCH 135
2024-05-10 16:05:07,404 Epoch 135: total training loss 0.00460
2024-05-10 16:05:07,404 EPOCH 136
2024-05-10 16:05:07,962 Epoch 136: total training loss 0.00459
2024-05-10 16:05:07,962 EPOCH 137
2024-05-10 16:05:08,514 Epoch 137: total training loss 0.00459
2024-05-10 16:05:08,514 EPOCH 138
2024-05-10 16:05:09,068 Epoch 138: total training loss 0.00459
2024-05-10 16:05:09,068 EPOCH 139
2024-05-10 16:05:09,620 Epoch 139: total training loss 0.00459
2024-05-10 16:05:09,620 EPOCH 140
2024-05-10 16:05:10,220 Epoch 140: total training loss 0.00459
2024-05-10 16:05:10,220 EPOCH 141
2024-05-10 16:05:10,810 Epoch 141: total training loss 0.00459
2024-05-10 16:05:10,810 EPOCH 142
2024-05-10 16:05:11,538 Epoch 142: total training loss 0.00459
2024-05-10 16:05:11,538 EPOCH 143
2024-05-10 16:05:12,147 Epoch 143: total training loss 0.00459
2024-05-10 16:05:12,148 EPOCH 144
2024-05-10 16:05:12,723 Epoch 144: total training loss 0.00459
2024-05-10 16:05:12,723 EPOCH 145
2024-05-10 16:05:13,285 Epoch 145: total training loss 0.00458
2024-05-10 16:05:13,285 EPOCH 146
2024-05-10 16:05:13,841 Epoch 146: total training loss 0.00458
2024-05-10 16:05:13,841 EPOCH 147
2024-05-10 16:05:14,402 Epoch 147: total training loss 0.00458
2024-05-10 16:05:14,402 EPOCH 148
2024-05-10 16:05:14,960 Epoch 148: total training loss 0.00458
2024-05-10 16:05:14,960 EPOCH 149
2024-05-10 16:05:15,523 Epoch 149: total training loss 0.00458
2024-05-10 16:05:15,523 EPOCH 150
2024-05-10 16:05:16,086 Epoch 150: total training loss 0.00458
2024-05-10 16:05:16,086 EPOCH 151
2024-05-10 16:05:16,645 Epoch 151: total training loss 0.00458
2024-05-10 16:05:16,645 EPOCH 152
2024-05-10 16:05:17,211 Epoch 152: total training loss 0.00458
2024-05-10 16:05:17,211 EPOCH 153
2024-05-10 16:05:17,821 Epoch 153: total training loss 0.00457
2024-05-10 16:05:17,821 EPOCH 154
2024-05-10 16:05:18,399 Epoch 154: total training loss 0.00457
2024-05-10 16:05:18,399 EPOCH 155
2024-05-10 16:05:18,979 Epoch 155: total training loss 0.00457
2024-05-10 16:05:18,979 EPOCH 156
2024-05-10 16:05:19,536 Epoch 156: total training loss 0.00457
2024-05-10 16:05:19,536 EPOCH 157
2024-05-10 16:05:20,317 Epoch 157: total training loss 0.00457
2024-05-10 16:05:20,317 EPOCH 158
2024-05-10 16:05:20,923 Epoch 158: total training loss 0.00457
2024-05-10 16:05:20,923 EPOCH 159
2024-05-10 16:05:21,501 Epoch 159: total training loss 0.00457
2024-05-10 16:05:21,501 EPOCH 160
2024-05-10 16:05:22,072 Epoch 160: total training loss 0.00457
2024-05-10 16:05:22,072 EPOCH 161
2024-05-10 16:05:22,647 Epoch 161: total training loss 0.00456
2024-05-10 16:05:22,647 EPOCH 162
2024-05-10 16:05:23,255 Epoch 162: total training loss 0.00456
2024-05-10 16:05:23,255 EPOCH 163
2024-05-10 16:05:24,340 Epoch 163: total training loss 0.00456
2024-05-10 16:05:24,340 EPOCH 164
2024-05-10 16:05:25,241 Epoch 164: total training loss 0.00456
2024-05-10 16:05:25,241 EPOCH 165
2024-05-10 16:05:25,821 Epoch 165: total training loss 0.00456
2024-05-10 16:05:25,821 EPOCH 166
2024-05-10 16:05:26,413 Epoch 166: total training loss 0.00456
2024-05-10 16:05:26,413 EPOCH 167
2024-05-10 16:05:27,162 Epoch 167: total training loss 0.00456
2024-05-10 16:05:27,163 EPOCH 168
2024-05-10 16:05:27,859 Epoch 168: total training loss 0.00455
2024-05-10 16:05:27,859 EPOCH 169
2024-05-10 16:05:28,677 Epoch 169: total training loss 0.00455
2024-05-10 16:05:28,677 EPOCH 170
2024-05-10 16:05:29,591 Epoch 170: total training loss 0.00455
2024-05-10 16:05:29,591 EPOCH 171
2024-05-10 16:05:30,295 Epoch 171: total training loss 0.00455
2024-05-10 16:05:30,295 EPOCH 172
2024-05-10 16:05:30,898 Epoch 172: total training loss 0.00455
2024-05-10 16:05:30,898 EPOCH 173
2024-05-10 16:05:31,477 Epoch 173: total training loss 0.00455
2024-05-10 16:05:31,477 EPOCH 174
2024-05-10 16:05:32,051 Epoch 174: total training loss 0.00454
2024-05-10 16:05:32,051 EPOCH 175
2024-05-10 16:05:32,620 Epoch 175: total training loss 0.00454
2024-05-10 16:05:32,620 EPOCH 176
2024-05-10 16:05:33,187 Epoch 176: total training loss 0.00454
2024-05-10 16:05:33,187 EPOCH 177
2024-05-10 16:05:33,743 Epoch 177: total training loss 0.00454
2024-05-10 16:05:33,743 EPOCH 178
2024-05-10 16:05:34,302 Epoch 178: total training loss 0.00454
2024-05-10 16:05:34,302 EPOCH 179
2024-05-10 16:05:34,850 Epoch 179: total training loss 0.00454
2024-05-10 16:05:34,850 EPOCH 180
2024-05-10 16:05:35,413 Epoch 180: total training loss 0.00453
2024-05-10 16:05:35,413 EPOCH 181
2024-05-10 16:05:35,975 Epoch 181: total training loss 0.00453
2024-05-10 16:05:35,975 EPOCH 182
2024-05-10 16:05:36,547 Epoch 182: total training loss 0.00453
2024-05-10 16:05:36,547 EPOCH 183
2024-05-10 16:05:37,765 Epoch 183: total training loss 0.00453
2024-05-10 16:05:37,766 EPOCH 184
2024-05-10 16:05:38,955 Epoch 184: total training loss 0.00453
2024-05-10 16:05:38,955 EPOCH 185
2024-05-10 16:05:40,267 Epoch 185: total training loss 0.00452
2024-05-10 16:05:40,267 EPOCH 186
2024-05-10 16:05:41,650 Epoch 186: total training loss 0.00452
2024-05-10 16:05:41,650 EPOCH 187
2024-05-10 16:05:42,270 Epoch 187: total training loss 0.00452
2024-05-10 16:05:42,270 EPOCH 188
2024-05-10 16:05:42,851 Epoch 188: total training loss 0.00452
2024-05-10 16:05:42,851 EPOCH 189
2024-05-10 16:05:43,448 Epoch 189: total training loss 0.00452
2024-05-10 16:05:43,448 EPOCH 190
2024-05-10 16:05:44,085 Epoch 190: total training loss 0.00451
2024-05-10 16:05:44,085 EPOCH 191
2024-05-10 16:05:44,676 Epoch 191: total training loss 0.00451
2024-05-10 16:05:44,676 EPOCH 192
2024-05-10 16:05:45,257 Epoch 192: total training loss 0.00451
2024-05-10 16:05:45,257 EPOCH 193
2024-05-10 16:05:45,829 Epoch 193: total training loss 0.00451
2024-05-10 16:05:45,830 EPOCH 194
2024-05-10 16:05:46,435 Epoch 194: total training loss 0.00450
2024-05-10 16:05:46,436 EPOCH 195
2024-05-10 16:05:47,006 Epoch 195: total training loss 0.00450
2024-05-10 16:05:47,006 EPOCH 196
2024-05-10 16:05:47,584 Epoch 196: total training loss 0.00450
2024-05-10 16:05:47,584 EPOCH 197
2024-05-10 16:05:48,158 Epoch 197: total training loss 0.00450
2024-05-10 16:05:48,158 EPOCH 198
2024-05-10 16:05:48,730 Epoch 198: total training loss 0.00449
2024-05-10 16:05:48,730 EPOCH 199
2024-05-10 16:05:49,309 Epoch 199: total training loss 0.00449
2024-05-10 16:05:49,309 EPOCH 200
2024-05-10 16:05:49,878 Epoch 200: total training loss 0.00449
2024-05-10 16:05:49,878 EPOCH 201
2024-05-10 16:05:50,430 Epoch 201: total training loss 0.00449
2024-05-10 16:05:50,430 EPOCH 202
2024-05-10 16:05:50,977 Epoch 202: total training loss 0.00448
2024-05-10 16:05:50,977 EPOCH 203
2024-05-10 16:05:51,535 Epoch 203: total training loss 0.00448
2024-05-10 16:05:51,535 EPOCH 204
2024-05-10 16:05:52,088 Epoch 204: total training loss 0.00448
2024-05-10 16:05:52,088 EPOCH 205
2024-05-10 16:05:52,648 Epoch 205: total training loss 0.00448
2024-05-10 16:05:52,648 EPOCH 206
2024-05-10 16:05:53,201 Epoch 206: total training loss 0.00447
2024-05-10 16:05:53,201 EPOCH 207
2024-05-10 16:05:53,750 Epoch 207: total training loss 0.00447
2024-05-10 16:05:53,750 EPOCH 208
2024-05-10 16:05:54,312 Epoch 208: total training loss 0.00447
2024-05-10 16:05:54,312 EPOCH 209
2024-05-10 16:05:54,876 Epoch 209: total training loss 0.00446
2024-05-10 16:05:54,876 EPOCH 210
2024-05-10 16:05:55,761 Epoch 210: total training loss 0.00446
2024-05-10 16:05:55,762 EPOCH 211
2024-05-10 16:05:56,810 Epoch 211: total training loss 0.00446
2024-05-10 16:05:56,810 EPOCH 212
2024-05-10 16:05:57,817 Epoch 212: total training loss 0.00445
2024-05-10 16:05:57,817 EPOCH 213
2024-05-10 16:05:58,892 Epoch 213: total training loss 0.00445
2024-05-10 16:05:58,892 EPOCH 214
2024-05-10 16:05:59,995 Epoch 214: total training loss 0.00445
2024-05-10 16:05:59,996 EPOCH 215
2024-05-10 16:06:01,000 Epoch 215: total training loss 0.00445
2024-05-10 16:06:01,001 EPOCH 216
2024-05-10 16:06:02,007 Epoch 216: total training loss 0.00444
2024-05-10 16:06:02,008 EPOCH 217
2024-05-10 16:06:03,038 Epoch 217: total training loss 0.00444
2024-05-10 16:06:03,039 EPOCH 218
2024-05-10 16:06:04,075 Epoch 218: total training loss 0.00444
2024-05-10 16:06:04,076 EPOCH 219
2024-05-10 16:06:05,129 Epoch 219: total training loss 0.00443
2024-05-10 16:06:05,130 EPOCH 220
2024-05-10 16:06:05,935 Epoch 220: total training loss 0.00443
2024-05-10 16:06:05,938 EPOCH 221
2024-05-10 16:06:06,507 Epoch 221: total training loss 0.00443
2024-05-10 16:06:06,507 EPOCH 222
2024-05-10 16:06:07,090 Epoch 222: total training loss 0.00442
2024-05-10 16:06:07,090 EPOCH 223
2024-05-10 16:06:07,679 Epoch 223: total training loss 0.00442
2024-05-10 16:06:07,679 EPOCH 224
2024-05-10 16:06:08,317 Epoch 224: total training loss 0.00441
2024-05-10 16:06:08,317 EPOCH 225
2024-05-10 16:06:08,895 Epoch 225: total training loss 0.00441
2024-05-10 16:06:08,896 EPOCH 226
2024-05-10 16:06:09,476 Epoch 226: total training loss 0.00441
2024-05-10 16:06:09,476 EPOCH 227
2024-05-10 16:06:10,059 Epoch 227: total training loss 0.00440
2024-05-10 16:06:10,059 EPOCH 228
2024-05-10 16:06:10,629 Epoch 228: total training loss 0.00440
2024-05-10 16:06:10,630 EPOCH 229
2024-05-10 16:06:11,201 Epoch 229: total training loss 0.00440
2024-05-10 16:06:11,201 EPOCH 230
2024-05-10 16:06:11,775 Epoch 230: total training loss 0.00439
2024-05-10 16:06:11,775 EPOCH 231
2024-05-10 16:06:12,356 Epoch 231: total training loss 0.00439
2024-05-10 16:06:12,357 EPOCH 232
2024-05-10 16:06:12,920 Epoch 232: total training loss 0.00438
2024-05-10 16:06:12,921 EPOCH 233
2024-05-10 16:06:13,494 Epoch 233: total training loss 0.00438
2024-05-10 16:06:13,494 EPOCH 234
2024-05-10 16:06:14,070 Epoch 234: total training loss 0.00438
2024-05-10 16:06:14,070 EPOCH 235
2024-05-10 16:06:14,662 Epoch 235: total training loss 0.00437
2024-05-10 16:06:14,662 EPOCH 236
2024-05-10 16:06:15,229 Epoch 236: total training loss 0.00437
2024-05-10 16:06:15,229 EPOCH 237
2024-05-10 16:06:15,790 Epoch 237: total training loss 0.00436
2024-05-10 16:06:15,790 EPOCH 238
2024-05-10 16:06:16,363 Epoch 238: total training loss 0.00436
2024-05-10 16:06:16,363 EPOCH 239
2024-05-10 16:06:16,938 Epoch 239: total training loss 0.00435
2024-05-10 16:06:16,939 EPOCH 240
2024-05-10 16:06:17,507 Epoch 240: total training loss 0.00435
2024-05-10 16:06:17,507 EPOCH 241
2024-05-10 16:06:18,095 Epoch 241: total training loss 0.00434
2024-05-10 16:06:18,095 EPOCH 242
2024-05-10 16:06:18,791 Epoch 242: total training loss 0.00434
2024-05-10 16:06:18,791 EPOCH 243
2024-05-10 16:06:19,360 Epoch 243: total training loss 0.00433
2024-05-10 16:06:19,360 EPOCH 244
2024-05-10 16:06:19,982 Epoch 244: total training loss 0.00433
2024-05-10 16:06:19,983 EPOCH 245
2024-05-10 16:06:20,620 Epoch 245: total training loss 0.00432
2024-05-10 16:06:20,621 EPOCH 246
2024-05-10 16:06:21,266 Epoch 246: total training loss 0.00432
2024-05-10 16:06:21,266 EPOCH 247
2024-05-10 16:06:21,852 Epoch 247: total training loss 0.00431
2024-05-10 16:06:21,852 EPOCH 248
2024-05-10 16:06:22,429 Epoch 248: total training loss 0.00431
2024-05-10 16:06:22,429 EPOCH 249
2024-05-10 16:06:23,025 Epoch 249: total training loss 0.00430
2024-05-10 16:06:23,025 EPOCH 250
