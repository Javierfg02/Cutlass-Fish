2024-05-10 15:01:02,285 Progressive Transformers for End-to-End SLP
2024-05-10 15:01:02,293 cfg.data.src                       : gloss
2024-05-10 15:01:02,293 cfg.data.trg                       : skels
2024-05-10 15:01:02,293 cfg.data.files                     : files
2024-05-10 15:01:02,293 cfg.data.train                     : ./data/train/raw/openpose_output/json/
2024-05-10 15:01:02,293 cfg.data.dev                       : ./data/val/raw/openpose_output/json/
2024-05-10 15:01:02,293 cfg.data.test                      : ./data/test/raw/openpose_output/json/
2024-05-10 15:01:02,293 cfg.data.max_sent_length           : 300
2024-05-10 15:01:02,293 cfg.data.skip_frames               : 1
2024-05-10 15:01:02,293 cfg.data.src_vocab                 : ./configs/src_vocab.txt
2024-05-10 15:01:02,293 cfg.training.random_seed           : 27
2024-05-10 15:01:02,293 cfg.training.optimizer             : adam
2024-05-10 15:01:02,293 cfg.training.learning_rate         : 0.001
2024-05-10 15:01:02,293 cfg.training.learning_rate_min     : 0.0002
2024-05-10 15:01:02,293 cfg.training.weight_decay          : 0.0
2024-05-10 15:01:02,293 cfg.training.clip_grad_norm        : 5.0
2024-05-10 15:01:02,293 cfg.training.batch_size            : 8
2024-05-10 15:01:02,293 cfg.training.scheduling            : plateau
2024-05-10 15:01:02,293 cfg.training.patience              : 7
2024-05-10 15:01:02,293 cfg.training.decrease_factor       : 0.7
2024-05-10 15:01:02,293 cfg.training.early_stopping_metric : dtw
2024-05-10 15:01:02,293 cfg.training.epochs                : 20
2024-05-10 15:01:02,293 cfg.training.validation_freq       : 10
2024-05-10 15:01:02,293 cfg.training.logging_freq          : 250
2024-05-10 15:01:02,293 cfg.training.eval_metric           : dtw
2024-05-10 15:01:02,293 cfg.training.model_dir             : ./Models/Base
2024-05-10 15:01:02,293 cfg.training.overwrite             : True
2024-05-10 15:01:02,293 cfg.training.continue              : False
2024-05-10 15:01:02,293 cfg.training.shuffle               : True
2024-05-10 15:01:02,293 cfg.training.use_cuda              : False
2024-05-10 15:01:02,294 cfg.training.max_output_length     : 420
2024-05-10 15:01:02,294 cfg.training.keep_last_ckpts       : 1
2024-05-10 15:01:02,294 cfg.training.loss                  : MSE
2024-05-10 15:01:02,294 cfg.model.initializer              : xavier
2024-05-10 15:01:02,294 cfg.model.bias_initializer         : zeros
2024-05-10 15:01:02,294 cfg.model.embed_initializer        : xavier
2024-05-10 15:01:02,294 cfg.model.trg_size                 : 412
2024-05-10 15:01:02,294 cfg.model.just_count_in            : False
2024-05-10 15:01:02,294 cfg.model.gaussian_noise           : False
2024-05-10 15:01:02,294 cfg.model.noise_rate               : 5
2024-05-10 15:01:02,294 cfg.model.future_prediction        : 0
2024-05-10 15:01:02,294 cfg.model.encoder.type             : transformer
2024-05-10 15:01:02,294 cfg.model.encoder.num_layers       : 2
2024-05-10 15:01:02,294 cfg.model.encoder.num_heads        : 4
2024-05-10 15:01:02,294 cfg.model.encoder.embeddings.embedding_dim : 512
2024-05-10 15:01:02,294 cfg.model.encoder.embeddings.dropout : 0.0
2024-05-10 15:01:02,294 cfg.model.encoder.hidden_size      : 512
2024-05-10 15:01:02,294 cfg.model.encoder.ff_size          : 2048
2024-05-10 15:01:02,294 cfg.model.encoder.dropout          : 0.0
2024-05-10 15:01:02,294 cfg.model.decoder.type             : transformer
2024-05-10 15:01:02,294 cfg.model.decoder.num_layers       : 2
2024-05-10 15:01:02,294 cfg.model.decoder.num_heads        : 4
2024-05-10 15:01:02,294 cfg.model.decoder.embeddings.embedding_dim : 512
2024-05-10 15:01:02,294 cfg.model.decoder.embeddings.dropout : 0.0
2024-05-10 15:01:02,294 cfg.model.decoder.hidden_size      : 512
2024-05-10 15:01:02,294 cfg.model.decoder.ff_size          : 2048
2024-05-10 15:01:02,294 cfg.model.decoder.dropout          : 0.0
2024-05-10 15:01:02,462 EPOCH 1
2024-05-10 15:01:03,536 Epoch   1: total training loss 1.88167
2024-05-10 15:01:03,536 EPOCH 2
2024-05-10 15:01:04,135 Epoch   2: total training loss 1.28583
2024-05-10 15:01:04,135 EPOCH 3
2024-05-10 15:01:04,707 Epoch   3: total training loss 0.33711
2024-05-10 15:01:04,707 EPOCH 4
2024-05-10 15:01:05,297 Epoch   4: total training loss 0.22342
2024-05-10 15:01:05,297 EPOCH 5
2024-05-10 15:01:05,859 Epoch   5: total training loss 0.10121
2024-05-10 15:01:05,859 EPOCH 6
2024-05-10 15:01:06,531 Epoch   6: total training loss 0.08835
2024-05-10 15:01:06,532 EPOCH 7
2024-05-10 15:01:07,097 Epoch   7: total training loss 0.08035
2024-05-10 15:01:07,097 EPOCH 8
2024-05-10 15:01:07,653 Epoch   8: total training loss 0.07987
2024-05-10 15:01:07,653 EPOCH 9
2024-05-10 15:01:08,216 Epoch   9: total training loss 0.07235
2024-05-10 15:01:08,216 EPOCH 10
2024-05-10 15:01:08,782 Epoch  10: total training loss 0.06188
2024-05-10 15:01:08,782 EPOCH 11
2024-05-10 15:01:09,334 Epoch  11: total training loss 0.05013
2024-05-10 15:01:09,335 EPOCH 12
2024-05-10 15:01:09,921 Epoch  12: total training loss 0.03676
2024-05-10 15:01:09,921 EPOCH 13
2024-05-10 15:01:10,533 Epoch  13: total training loss 0.02586
2024-05-10 15:01:10,533 EPOCH 14
2024-05-10 15:01:11,090 Epoch  14: total training loss 0.02203
2024-05-10 15:01:11,090 EPOCH 15
2024-05-10 15:01:11,663 Epoch  15: total training loss 0.02423
2024-05-10 15:01:11,663 EPOCH 16
2024-05-10 15:01:12,237 Epoch  16: total training loss 0.02731
2024-05-10 15:01:12,238 EPOCH 17
2024-05-10 15:01:12,833 Epoch  17: total training loss 0.02770
2024-05-10 15:01:12,833 EPOCH 18
2024-05-10 15:01:13,557 Epoch  18: total training loss 0.02563
2024-05-10 15:01:13,557 EPOCH 19
2024-05-10 15:01:14,116 Epoch  19: total training loss 0.02282
2024-05-10 15:01:14,116 EPOCH 20
2024-05-10 15:01:14,671 Epoch  20: total training loss 0.02002
2024-05-10 15:01:14,672 Training ended after  20 epochs.
